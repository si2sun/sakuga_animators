import os
import sys
import shutil

# æ·»åŠ è·¯å¾‘ä»¥å°å…¥æ¨¡å¡Š
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from configs.base_config import BaseConfig
from core.data_manager import DataManager
from core.model_trainerv5 import ModelTrainer

def main():
    # äº¤äº’å¼é¸æ“‡æ¨¡å¼
    mode_input = input("è«‹å•è¦é€²å…¥å“ªä¸€å€‹æ¨¡å‹ 1.é‡é ­è¨“ç·´ 2.æ–°è³‡æ–™è¨“ç·´ 3.å¼±é¡åˆ¥é‡å°è¨“ç·´: ").strip()
    
    if mode_input == '1':
        mode = 'from_scratch'
    elif mode_input == '2':
        mode = 'incremental'
    elif mode_input == '3':
        mode = 'fine_tune_weak'
    else:
        print("âŒ ç„¡æ•ˆè¼¸å…¥ï¼Œè«‹é¸æ“‡ 1, 2 æˆ– 3ã€‚")
        return

    print(f"ğŸ¯ é–‹å§‹æ¨¡å‹è¨“ç·´æµç¨‹ (æ¨¡å¼: {mode})")

    # Early Stopping é è¨­
    early_stopping_patience = 3
    early_stopping_monitor = 'test_acc'
    print(f"ğŸ›‘ Early Stopping è¨­ç½®: patience={early_stopping_patience}, monitor={early_stopping_monitor}")

    # --- 2. åŸºç¤é…ç½® ---
    config = BaseConfig()
    data_manager = DataManager(config)
    data_manager.create_fixed_train_test_split(train_ratio=0.8, force_resplit=False)
    print("âœ… å›ºå®šè¨“ç·´/æ¸¬è©¦åˆ†å‰²å·²å‰µå»ºæˆ–ç¢ºèª")

    feature_samples = data_manager.get_all_feature_samples()
    if not feature_samples:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç‰¹å¾µæ–‡ä»¶ã€‚è«‹ç¢ºä¿å·²ä½¿ç”¨ 'extract_features_finetuned.py' ç”Ÿæˆæ–°çš„ .npy æª”æ¡ˆã€‚")
        return

    print(f"ğŸ“Š ä½¿ç”¨ {len(feature_samples)} å€‹ã€é«˜è³ªé‡ã€‘ç‰¹å¾µé€²è¡Œè¨“ç·´")
    all_animators = list(data_manager.label_to_idx.keys())
    print(f"ğŸ¨ æ‰€æœ‰åŸç•«å¸«: {all_animators}")

    # --- 3. æ ¹æ“šä¸åŒæ¨¡å¼åŸ·è¡Œä¸åŒé‚è¼¯ ---

    if mode == 'from_scratch':
        print("\n==============================")
        print("ğŸ”¥ æ¨¡å¼: å¾é ­è¨“ç·´ (From Scratch)")
        print("   - ç›®æ¨™: é©—è­‰æ–°ç‰¹å¾µçš„æ½›åŠ›ä¸Šé™")
        print("==============================")

        trainer = ModelTrainer(
            config,
            data_manager,
            use_incremental=False,
            use_kd=False,
            new_animators=None,
            early_stopping_patience=early_stopping_patience,
            early_stopping_monitor=early_stopping_monitor
        )
        
        model, history = trainer.train_model()
        
        print("\nğŸ å¾é ­è¨“ç·´å®Œæˆ!")
        print("   - é€™å€‹æ¨¡å‹çš„æœ€ä½³æº–ç¢ºç‡ï¼Œä»£è¡¨äº†æ‚¨ç•¶å‰æ•¸æ“šå’Œæ¶æ§‹çš„æ€§èƒ½å¤©èŠ±æ¿ã€‚")
        print("   - best_model.pth å’Œ latest_model.pth å·²æ›´æ–°ç‚ºæ­¤æ¨¡å‹çš„æ¬Šé‡ã€‚")

    elif mode == 'incremental':
        print("\n==============================")
        print("ğŸ§  æ¨¡å¼: å¢é‡å­¸ç¿’ (Incremental)")
        print("   - ç›®æ¨™: åœ¨ä¿ç•™èˆŠçŸ¥è­˜çš„åŸºç¤ä¸Šï¼Œé«˜æ•ˆå­¸ç¿’æ–°é¡åˆ¥")
        print("==============================")
        
        old_model_path = os.path.join(config.model_dir, 'best_model.pth')
        teacher_model_path = os.path.join(config.model_dir, 'train_model.pth')

        if not os.path.exists(old_model_path):
            print(f"âŒ éŒ¯èª¤: å¢é‡å­¸ç¿’éœ€è¦ä¸€å€‹ä»£è¡¨èˆŠçŸ¥è­˜çš„æ¨¡å‹ã€‚")
            print(f"   è«‹æä¾›ä¸€å€‹åœ¨èˆŠé¡åˆ¥ä¸Šè¨“ç·´å¥½çš„æ¨¡å‹ï¼Œä¸¦å°‡å…¶å‘½åç‚º '{os.path.basename(old_model_path)}' æ”¾åœ¨ 'models' è³‡æ–™å¤¾ä¸­ã€‚")
            return
            
        print(f"ğŸ“š æ‰¾åˆ°èˆŠæ¨¡å‹ {old_model_path}ï¼Œå°‡å…¶ä½œç‚ºæœ¬æ¬¡å¢é‡å­¸ç¿’çš„ Teacherã€‚")
        shutil.copyfile(old_model_path, teacher_model_path)

        # é è¨­æ–°é¡åˆ¥
        new_animators = ['å‰æˆæ›œ','ç”°ä¸­å®ç´€']
        
        trainer = ModelTrainer(
            config,
            data_manager,
            use_incremental=True,
            replay_ratio=0.7,
            lambda_kd=1.0,
            kd_temperature=2.5,
            head_warmup_epochs=5,
            new_animators=new_animators,
            early_stopping_patience=early_stopping_patience,
            early_stopping_monitor=early_stopping_monitor
        )
        
        model, history = trainer.train_model()

        print("\nğŸ å¢é‡å­¸ç¿’è¨“ç·´å®Œæˆ!")

    elif mode == 'fine_tune_weak':
        print("\n==============================")
        print("ğŸ› ï¸ æ¨¡å¼: é‡å°å¼±é¡åˆ¥å¾®èª¿ (Fine-Tune Weak Classes)")
        print("   - ç›®æ¨™: å„ªåŒ–ç‰¹å®šåŸç•«å¸«çš„æ€§èƒ½ï¼Œä¿ç•™æ•´é«”çŸ¥è­˜")
        print("==============================")

        # é è¨­å¼±é¡åˆ¥
        weak_animators = ['åœ‹å¼˜æ˜Œä¹‹','å±±ä¸‹å®å¹¸']
        print(f"ğŸ–Œï¸ é è¨­å¼±é¡åˆ¥: {weak_animators}")

        if not weak_animators:
            print("âŒ éŒ¯èª¤: fine_tune_weak æ¨¡å¼éœ€è¦æŒ‡å®šå¼±é¡åˆ¥")
            return

        invalid_animators = [anim for anim in weak_animators if anim not in all_animators]
        if invalid_animators:
            print(f"âŒ éŒ¯èª¤: ä»¥ä¸‹åŸç•«å¸«ä¸åœ¨æ•¸æ“šé›†ä¸­: {invalid_animators}")
            return

        old_model_path = os.path.join(config.model_dir, 'best_model.pth')
        teacher_model_path = os.path.join(config.model_dir, 'teacher_model.pth')
        if not os.path.exists(old_model_path):
            print(f"âŒ éŒ¯èª¤: éœ€è¦ç¾æœ‰æ¨¡å‹ {old_model_path}")
            return
        print(f"ğŸ“š æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ {old_model_path}ï¼Œå°‡å…¶ä½œç‚º Teacherã€‚")
        shutil.copyfile(old_model_path, teacher_model_path)

        trainer = ModelTrainer(
            config,
            data_manager,
            use_incremental=True,
            replay_ratio=0.7,
            lambda_kd=1.0,
            kd_temperature=3.0,
            head_warmup_epochs=3,
            new_animators=weak_animators,
            early_stopping_patience=early_stopping_patience,
            early_stopping_monitor=early_stopping_monitor
        )
        
        model, history = trainer.train_model()

        print("\nğŸ å¼±é¡åˆ¥å¾®èª¿å®Œæˆ!")
        print("   - æª¢æŸ¥è¼¸å‡ºä¸­çš„èˆŠ/æ–°é¡æº–ç¢ºç‡ï¼ˆæ–°é¡å³æ‚¨çš„å¼±é¡åˆ¥ï¼‰ã€‚")

if __name__ == '__main__':
    main()
