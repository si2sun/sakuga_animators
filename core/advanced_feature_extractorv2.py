import os
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from PIL import Image
import cv2
import torchvision.transforms as transforms
import torch.nn.functional as F  # â­ æ–°å¢ï¼šF for conv2d ç­‰

# â­ å˜—è©¦å°å…¥ Decord (å¿«é€Ÿ)
try:
    from decord import VideoReader, cpu
    DECORD_AVAILABLE = True
    print("âœ… Decord å¯ç”¨ - å°‡ä½¿ç”¨å¿«é€Ÿæ¨¡å¼")
except ImportError:
    DECORD_AVAILABLE = False
    print("âš ï¸  Decord æœªå®‰è£ - ä½¿ç”¨ OpenCV (è¼ƒæ…¢)")
    print("ğŸ’¡ å®‰è£ Decord å¯æé€Ÿ 3-5å€: pip install decord")

def detect_aspect_ratio(video_path):
    """æª¢æ¸¬å½±ç‰‡æ¯”ä¾‹ï¼ˆç¨ç«‹å‡½æ•¸ï¼Œé¿å…å¾ªç’°å°å…¥ï¼‰"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return "unknown"
    
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()
    
    if width == 0 or height == 0:
        return "unknown"
    
    aspect_ratio = width / height
    
    if abs(aspect_ratio - 16/9) < 0.1:
        return "16:9"
    elif abs(aspect_ratio - 4/3) < 0.1:
        return "4:3"
    elif abs(aspect_ratio - 1) < 0.1:
        return "1:1"
    elif abs(aspect_ratio - 936/480) < 0.1:
        return "936:480"
    elif abs(aspect_ratio - 1136/480) < 0.1:
        return "1136:480"
    else:
        return "other"

class PytorchCanny(nn.Module):
    """
    PyTorch å¯¦ç¾çš„ Canny é‚Šç·£æª¢æ¸¬ (GPU åŠ é€Ÿ)
    è¿”å›: edges (äºŒå€¼é‚Šç·£åœ–), mag (æ¢¯åº¦å¹…åº¦), ang (æ¢¯åº¦è§’åº¦, åº¦æ•¸ 0-180)
    """
    def __init__(self, low_threshold=50, high_threshold=150):
        super(PytorchCanny, self).__init__()
        self.low = low_threshold
        self.high = high_threshold

        # é«˜æ–¯æ ¸ (5x5, sigma=1)
        k = 5
        gaussian = torch.tensor([
            [2, 4, 5, 4, 2],
            [4, 9, 12, 9, 4],
            [5, 12, 15, 12, 5],
            [4, 9, 12, 9, 4],
            [2, 4, 5, 4, 2]
        ], dtype=torch.float32).view(1, 1, k, k) / 159.0
        self.register_buffer('gaussian', gaussian)

        # Sobel æ ¸
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3) / 8.0
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3) / 8.0
        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)

    def forward(self, x):  # x: [B, 1, H, W] uint8 (0-255)
        x = x.float() / 255.0  # æ­£è¦åŒ–

        # é«˜æ–¯å¹³æ»‘
        smoothed = F.conv2d(x, self.gaussian, padding=2)

        # æ¢¯åº¦
        gx = F.conv2d(smoothed, self.sobel_x, padding=1)
        gy = F.conv2d(smoothed, self.sobel_y, padding=1)
        mag = torch.sqrt(gx**2 + gy**2 + 1e-6)
        ang = torch.atan2(gy, gx)  # å¼§åº¦ (-pi ~ pi)

        # éæœ€å¤§æŠ‘åˆ¶
        mag_sup = self._non_max_suppression(mag, ang)

        # é›™é–¾å€¼é²æ»¯
        high = (mag_sup > self.high / 255.0).float()
        low = ((mag_sup >= self.low / 255.0) & (mag_sup < self.high / 255.0)).float()

        # é²æ»¯ï¼šå¼±é‚Šé€£é€šå¼·é‚Š (ç”¨ 3x3 é„°åŸŸæª¢æŸ¥ï¼Œè¿­ä»£ 2 æ¬¡)
        edges = high.clone()
        kernel = torch.ones(1, 1, 3, 3, device=x.device)
        for _ in range(2):
            neighbor_count = F.conv2d(edges, kernel, padding=1)
            connected = (neighbor_count > 0).float() * low
            edges = edges + connected
            low = low * (1 - connected)
        
        edges = torch.clamp(edges, 0, 1)  # ç¢ºä¿ [0,1]
        edges = (edges * 255).byte()  # [B, 1, H, W] 0/255
        mag = mag * 255  # è½‰å› 0-255 ç¯„åœ
        ang = torch.rad2deg((ang + torch.pi) % (2 * torch.pi)) % 180  # å¼§åº¦ â†’ åº¦æ•¸ 0-180Â°

        return edges, mag, ang

    def _non_max_suppression(self, mag, ang):
        # å‘é‡åŒ–å¯¦ç¾ (é¿å…æ…¢è¿´åœˆ)
        b, _, h, w = mag.shape
        mag_sup = torch.zeros_like(mag)

        # è§’åº¦é‡åŒ–åˆ° 0/45/90/135Â°
        ang_deg = torch.rad2deg((ang + torch.pi) % (2 * torch.pi))  # 0-360
        angle_quant = torch.round(ang_deg / 45) % 8 * 45  # 0/45/.../315

        # é‚Šç•Œè™•ç† (ç°¡å–®è¨­ 0)
        mag_sup[:, :, 0, :] = 0
        mag_sup[:, :, -1, :] = 0
        mag_sup[:, :, :, 0] = 0
        mag_sup[:, :, :, -1] = 0

        # æ°´å¹³ (0Â°/180Â°)
        mask_h = (angle_quant % 180 < 22.5) | (angle_quant % 180 > 157.5)
        n_right = torch.roll(mag, shifts=-1, dims=3)
        n_left = torch.roll(mag, shifts=1, dims=3)
        is_max_h = (mag > n_right) & (mag > n_left)
        mag_sup[mask_h & is_max_h] = mag[mask_h & is_max_h]

        # å‚ç›´ (90Â°)
        mask_v = (angle_quant % 180 >= 67.5) & (angle_quant % 180 < 112.5)
        n_down = torch.roll(mag, shifts=-1, dims=2)
        n_up = torch.roll(mag, shifts=1, dims=2)
        is_max_v = (mag > n_down) & (mag > n_up)
        mag_sup[mask_v & is_max_v] = mag[mask_v & is_max_v]

        # 45Â°
        mask_d1 = (angle_quant % 180 >= 22.5) & (angle_quant % 180 < 67.5)
        n_diag1 = torch.roll(torch.roll(mag, shifts=-1, dims=2), shifts=-1, dims=3)
        n_diag2 = torch.roll(torch.roll(mag, shifts=1, dims=2), shifts=1, dims=3)
        is_max_d1 = (mag > n_diag1) & (mag > n_up)
        mag_sup[mask_d1 & is_max_d1] = mag[mask_d1 & is_max_d1]

        # 135Â°
        mask_d2 = (angle_quant % 180 >= 112.5) & (angle_quant % 180 < 157.5)
        n_diag3 = torch.roll(torch.roll(mag, shifts=-1, dims=2), shifts=1, dims=3)
        n_diag4 = torch.roll(torch.roll(mag, shifts=1, dims=2), shifts=-1, dims=3)
        is_max_d2 = (mag > n_diag3) & (mag > n_diag4)
        mag_sup[mask_d2 & is_max_d2] = mag[mask_d2 & is_max_d2]

        return mag_sup

class FeatureExtractor:
    def __init__(self, config):
        self.config = config
        self.use_decord = DECORD_AVAILABLE
        self.setup_feature_extractor()
    
    def setup_feature_extractor(self):
        """è¨­ç½®ç‰¹å¾µæå–å™¨"""
        from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights
        
        print("ğŸ”¥ è¼‰å…¥ EfficientNetV2-S é€²è¡Œç‰¹å¾µæå–...")
        efficientnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)
        self.feature_extractor = efficientnet.features
        self.feature_extractor.eval()
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # å‡çµæ¬Šé‡
        for param in self.feature_extractor.parameters():
            param.requires_grad = False
        
        self.feature_extractor = self.feature_extractor.to(self.config.device)
        
        # â­ åˆå§‹åŒ– PyTorch Canny (GPU åŠ é€Ÿ)
        self.canny_model = PytorchCanny(low_threshold=50, high_threshold=150).to(self.config.device)
        
        # ä½¿ç”¨æ™ºèƒ½èª¿æ•´å¤§å°çš„è®Šæ›
        target_size = (self.config.target_height, self.config.target_width)
        self.transform = transforms.Compose([
            transforms.Lambda(lambda img: self._smart_resize(img, target_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # â­ é ç†±æ¨¡å‹ (åŠ é€Ÿé¦–æ¬¡é‹è¡Œ)
        if self.config.device.type == 'cuda':
            with torch.no_grad():
                dummy_input = torch.randn(1, 3, self.config.target_height, self.config.target_width).to(self.config.device)
                self.feature_extractor(dummy_input)
                self.canny_model(torch.zeros(1, 1, self.config.target_height, self.config.target_width, dtype=torch.uint8).to(self.config.device))
            torch.cuda.empty_cache()
    
    def _smart_resize(self, image, target_size):
        """æ™ºèƒ½èª¿æ•´å¤§å°ï¼šå„ªå…ˆç¸®æ”¾ï¼Œå¿…è¦æ™‚æ™ºèƒ½è£åˆ‡ï¼ˆä¿ç•™92%ä¸­å¿ƒå…§å®¹ï¼‰"""
        target_h, target_w = target_size
        orig_w, orig_h = image.size
        
        target_ratio = target_w / target_h
        orig_ratio = orig_w / orig_h
        
        # ç²å–é…ç½®åƒæ•¸
        content_retain = getattr(self.config, 'content_retain', 0.92)
        tolerance = getattr(self.config, 'aspect_ratio_tolerance', 0.08)
        
        # å¦‚æœæ¯”ä¾‹æ¥è¿‘ï¼ˆèª¤å·®<8%ï¼‰ï¼Œç›´æ¥ç¸®æ”¾ï¼ˆç„¡è£åˆ‡ï¼‰
        if abs(target_ratio - orig_ratio) / orig_ratio < tolerance:
            return image.resize((target_w, target_h), Image.LANCZOS)
        
        # æ¯”ä¾‹å·®ç•°å¤§æ™‚ï¼Œä½¿ç”¨æ™ºèƒ½è£åˆ‡
        if orig_ratio > target_ratio:
            # åŸåœ–æ›´å¯¬ï¼Œéœ€è¦è£åˆ‡å·¦å³
            scale = target_h / orig_h
            new_w = int(orig_w * scale)
            new_h = target_h
            resized = image.resize((new_w, new_h), Image.LANCZOS)
            
            # è¨ˆç®—ä¿ç•™å€åŸŸï¼ˆä¸­å¿ƒ92%ï¼‰
            crop_w = int(new_w * content_retain)
            crop_w = min(crop_w, target_w)
            
            # å±…ä¸­è£åˆ‡
            left = (new_w - crop_w) // 2
            cropped = resized.crop((left, 0, left + crop_w, new_h))
            
            # å¦‚æœè£åˆ‡å¾Œä»ä¸æ˜¯ç›®æ¨™å°ºå¯¸ï¼Œå†ç¸®æ”¾
            if crop_w != target_w:
                return cropped.resize((target_w, target_h), Image.LANCZOS)
            return cropped
        else:
            # åŸåœ–æ›´é«˜ï¼Œéœ€è¦è£åˆ‡ä¸Šä¸‹
            scale = target_w / orig_w
            new_w = target_w
            new_h = int(orig_h * scale)
            resized = image.resize((new_w, new_h), Image.LANCZOS)
            
            # è¨ˆç®—ä¿ç•™å€åŸŸï¼ˆä¸­å¿ƒ92%ï¼‰
            crop_h = int(new_h * content_retain)
            crop_h = min(crop_h, target_h)
            
            # å±…ä¸­è£åˆ‡
            top = (new_h - crop_h) // 2
            cropped = resized.crop((0, top, new_w, top + crop_h))
            
            # å¦‚æœè£åˆ‡å¾Œä»ä¸æ˜¯ç›®æ¨™å°ºå¯¸ï¼Œå†ç¸®æ”¾
            if crop_h != target_h:
                return cropped.resize((target_w, target_h), Image.LANCZOS)
            return cropped
    
    def _extract_edge_features(self, frame_rgb):
        """
        â­ æ–°å¢ï¼šæå–é‚Šç·£ç‰¹å¾µï¼ˆ64ç¶­ï¼‰ï¼Œä½¿ç”¨ PyTorch Canny (GPU åŠ é€Ÿ)
        
        ç‰¹å¾µçµ„æˆ:
        - é‚Šç·£å¯†åº¦ (1ç¶­)
        - é‚Šç·£æ–¹å‘ç›´æ–¹åœ– (8ç¶­, 0-180Â°)
        - é‚Šç·£å¼·åº¦çµ±è¨ˆ (mean, std, max, min, 4ç¶­)
        - è¼ªå»“çµ±è¨ˆ (æ•¸é‡, ç¸½é•·åº¦, å¹³å‡é¢ç©, å¡«å……ç‡, 4ç¶­)
        - å¡«å……åˆ°64ç¶­
        
        Args:
            frame_rgb: (H, W, 3) numpy array
            
        Returns:
            edge_feat: (64,) numpy array
        """
        # è½‰ç°åº¦ (CPU, å¿«)
        gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)
        h, w = gray.shape
        
        # PyTorch Canny + Sobel (GPU)
        gray_tensor = torch.from_numpy(gray).unsqueeze(0).unsqueeze(0).byte().to(self.config.device)  # [1,1,H,W]
        edges_tensor, mag_tensor, ang_tensor = self.canny_model(gray_tensor)
        edges = edges_tensor.squeeze().cpu().numpy()  # [H, W] 0/255
        mag = mag_tensor.squeeze().cpu().numpy()  # [H, W] 0-255
        ang_deg = ang_tensor.squeeze().cpu().numpy()  # [H, W] 0-180Â°
        
        # 1. é‚Šç·£å¯†åº¦
        edge_density = np.sum(edges > 0) / (h * w)
        features = [edge_density]
        
        # 2. é‚Šç·£æ–¹å‘ç›´æ–¹åœ– (ä½¿ç”¨ PyTorch ang åœ¨é‚Šç·£è™•)
        hist, _ = np.histogram(ang_deg[edges > 0], bins=8, range=(0, 180))
        hist = hist.astype(np.float32) / (np.sum(hist) + 1e-6)  # æ­¸ä¸€åŒ–
        features.extend(hist.tolist())
        
        # 3. é‚Šç·£å¼·åº¦çµ±è¨ˆ (mag åœ¨é‚Šç·£è™•)
        edge_mags = mag[edges > 0]
        if len(edge_mags) > 0:
            features.extend([
                np.mean(edge_mags),
                np.std(edge_mags),
                np.max(edge_mags),
                np.min(edge_mags)
            ])
        else:
            features.extend([0.0, 0.0, 0.0, 0.0])
        
        # 4. è¼ªå»“çµ±è¨ˆ (ä»ç”¨ OpenCV, CPU)
        contours, _ = cv2.findContours(edges.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        num_contours = len(contours)
        total_perimeter = sum(cv2.arcLength(c, True) for c in contours)
        total_area = sum(cv2.contourArea(c) for c in contours) if contours else 0
        avg_area = total_area / max(num_contours, 1)
        fill_ratio = total_area / (h * w + 1e-6)
        features.extend([num_contours / 100.0, total_perimeter / (h * w), avg_area / (h * w), fill_ratio])
        
        # å¡«å……åˆ°64ç¶­
        edge_feat = np.array(features, dtype=np.float32)
        if len(edge_feat) < 64:
            edge_feat = np.pad(edge_feat, (0, 64 - len(edge_feat)))
        else:
            edge_feat = edge_feat[:64]
        
        return edge_feat
    
    def _detect_aspect_ratio(self, video_path):
        """æª¢æ¸¬å½±ç‰‡æ¯”ä¾‹ï¼ˆä½¿ç”¨ç¨ç«‹å‡½æ•¸ï¼‰"""
        return detect_aspect_ratio(video_path)
    
    def _calculate_sample_indices(self, total_frames, fps):
        """
        â­ è¨ˆç®—æ¡æ¨£å¹€ç´¢å¼•ï¼ˆå‹•æ…‹é•·åº¦ï¼‰
        
        Returns:
            sample_indices: æ¡æ¨£å¹€çš„ç´¢å¼•åˆ—è¡¨
            actual_frames: å¯¦éš›æ¡æ¨£å¹€æ•¸
            process_duration: å¯¦éš›è™•ç†æ™‚é•·
        """
        if fps <= 0:
            fps = 24  # é»˜èª24fps
        
        # è¨ˆç®—è¦–é »å¯¦éš›æ™‚é•·
        video_duration = total_frames / fps
        
        # ç²å–é…ç½®åƒæ•¸
        target_fps = getattr(self.config, 'target_fps', 12)
        min_duration = getattr(self.config, 'min_video_duration', 2)
        max_duration = getattr(self.config, 'max_video_duration', 15)
        
        # é™åˆ¶è™•ç†ç¯„åœ
        process_duration = max(min_duration, min(video_duration, max_duration))
        
        # â­ å‹•æ…‹è¨ˆç®—ç›®æ¨™å¹€æ•¸ï¼ˆè€Œéå›ºå®šå€¼ï¼‰
        target_frames = int(process_duration * target_fps)
        
        # è¨ˆç®—å¹€é–“éš”
        frame_interval = fps / target_fps
        
        # ç”Ÿæˆæ¡æ¨£ç´¢å¼•
        sample_indices = []
        current_pos = 0.0
        
        while len(sample_indices) < target_frames and int(current_pos) < total_frames:
            sample_indices.append(int(current_pos))
            current_pos += frame_interval
        
        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å¹€
        if not sample_indices:
            sample_indices = [0]
        
        return sample_indices, len(sample_indices), process_duration
    
    def _extract_edge_features_batch(self, frames_rgb_list):
        """
        â­ æ‰¹é‡æå–è¾¹ç¼˜ç‰¹å¾ (GPUåŠ é€Ÿ) - ä¸æ”¹å˜æ•°å€¼
        
        Args:
            frames_rgb_list: List of (H, W, 3) numpy arrays
            
        Returns:
            edge_features: (N, 64) numpy array
        """
        batch_size = 32  # å¢å¤§batch
        all_edge_features = []
        
        for i in range(0, len(frames_rgb_list), batch_size):
            batch_frames = frames_rgb_list[i:i+batch_size]
            
            # 1. æ‰¹é‡è½¬ç°åº¦ (CPU, å¿«é€Ÿ) - é å…ˆè½‰uint8é¿å…å¾ŒçºŒè½‰å‹
            gray_batch = [cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY).astype(np.uint8) for frame in batch_frames]  # æå‰astype
            h, w = gray_batch[0].shape
            
            # 2. æ‰¹é‡è½¬GPU tensor [B, 1, H, W] - ç›´æ¥å¾uint8
            gray_tensors = torch.from_numpy(np.stack(gray_batch)).unsqueeze(1).byte().to(self.config.device)  # stackä¸€æ¬¡ï¼Œå°‘è½‰æ›
            
            # 3. æ‰¹é‡Cannyæ£€æµ‹ (GPU)
            edges_batch, mag_batch, ang_batch = self.canny_model(gray_tensors)
            
            # 4. æ‰¹é‡æå–ç‰¹å¾ - é å…ˆè½‰å›CPU/numpyï¼Œæ‰¹é‡è™•ç†çµ±è¨ˆ
            edges_np = edges_batch.squeeze(1).cpu().numpy().astype(np.uint8)  # [B, H, W] uint8ï¼Œä¸€æ¬¡è½‰
            mag_np = mag_batch.squeeze(1).cpu().numpy()  # [B, H, W] float
            ang_deg_np = ang_batch.squeeze(1).cpu().numpy()  # [B, H, W] float
            
            batch_features = []
            for j in range(len(batch_frames)):
                edges = edges_np[j]
                mag = mag_np[j]
                ang_deg = ang_deg_np[j]
                
                # é‚Šç·£å¯†åº¦ & æ–¹å‘ç›´æ–¹åœ– - å‘é‡åŒ–ï¼ˆnumpyå¿«ï¼‰
                edge_mask = edges > 0
                edge_density = np.sum(edge_mask) / (h * w)
                features = [edge_density]
                
                if np.sum(edge_mask) > 0:
                    hist, _ = np.histogram(ang_deg[edge_mask], bins=8, range=(0, 180))
                    hist = hist.astype(np.float32) / (np.sum(hist) + 1e-6)
                else:
                    hist = np.zeros(8, dtype=np.float32)
                features.extend(hist.tolist())
                
                # å¼·åº¦çµ±è¨ˆ - å‘é‡åŒ–
                edge_mags = mag[edge_mask]
                if len(edge_mags) > 0:
                    features.extend([
                        np.mean(edge_mags), np.std(edge_mags),
                        np.max(edge_mags), np.min(edge_mags)
                    ])
                else:
                    features.extend([0.0, 0.0, 0.0, 0.0])
                
                # è¼ªå»“çµ±è¨ˆ - å„ªåŒ–ï¼šå–®å¹€cv2ï¼Œé è¨ˆç®—ç¸½å’Œé¿å…é‡è¤‡sum
                contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                num_contours = len(contours)
                if num_contours > 0:
                    perimeters = [cv2.arcLength(c, True) for c in contours]  # é å­˜list
                    areas = [cv2.contourArea(c) for c in contours]
                    total_perimeter = sum(perimeters)
                    total_area = sum(areas)
                    avg_area = total_area / num_contours  # ç›´æ¥é™¤ï¼Œé¿å…max(1)
                else:
                    total_perimeter = total_area = avg_area = 0.0
                fill_ratio = total_area / (h * w + 1e-6)
                features.extend([num_contours / 100.0, total_perimeter / (h * w), avg_area / (h * w), fill_ratio])
                
                # å¡«å……åˆ°64ç¶­
                edge_feat = np.array(features, dtype=np.float32)
                if len(edge_feat) < 64:
                    edge_feat = np.pad(edge_feat, (0, 64 - len(edge_feat)))
                else:
                    edge_feat = edge_feat[:64]
                
                batch_features.append(edge_feat)
            
            all_edge_features.extend(batch_features)
        
        return np.array(all_edge_features)


    def extract_video_features_decord(self, video_path):
        """
        â­ ä¼˜åŒ–ç‰ˆ Decord æå– (æ‰¹é‡è¾¹ç¼˜ç‰¹å¾)
        """
        try:
            vr = VideoReader(video_path, ctx=cpu(0))
            total_frames = len(vr)
            fps = 24
            
            aspect_ratio = self._detect_aspect_ratio(video_path)
            duration = total_frames / fps
            
            print(f"  ğŸ“¹ {os.path.basename(video_path)}")
            print(f"     æ¯”ä¾‹: {aspect_ratio} | FPS: {fps} | é•¿åº¦: {duration:.1f}ç§’")
            
            sample_indices, actual_frames, process_duration = self._calculate_sample_indices(total_frames, fps)
            target_fps = getattr(self.config, 'target_fps', 12)
            print(f"     é‡‡æ ·: {actual_frames} å¸§ ({process_duration:.1f}ç§’ @ {target_fps}FPS)")
            
            # è¯»å–å¸§
            frames_rgb = []
            for idx in sample_indices:
                if idx < total_frames:
                    frame = vr[idx].asnumpy()
                    if len(frame.shape) == 3:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    else:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
                    frames_rgb.append(frame_rgb)
                else:
                    if frames_rgb:
                        frames_rgb.append(frames_rgb[-1])
                    else:
                        h, w = self.config.target_height, self.config.target_width
                        frames_rgb.append(np.zeros((h, w, 3), dtype=np.uint8))
            
            # â­ æ‰¹é‡æå– EfficientNet ç‰¹å¾
            frames_pil = [Image.fromarray(frame) for frame in frames_rgb]
            frame_tensors = [self.transform(pil) for pil in frames_pil]
            
            efficientnet_features = []
            batch_size = 32  # å¢å¤§batch
            
            with torch.no_grad():
                for i in range(0, len(frame_tensors), batch_size):
                    batch_frames = frame_tensors[i:i+batch_size]
                    batch_tensor = torch.stack(batch_frames).to(self.config.device)
                    
                    batch_features = self.feature_extractor(batch_tensor)
                    batch_features = self.global_pool(batch_features)
                    batch_features = batch_features.flatten(1).cpu().numpy()
                    
                    efficientnet_features.extend(batch_features)
            
            efficientnet_array = np.array(efficientnet_features)
            
            # â­ æ‰¹é‡æå–è¾¹ç¼˜ç‰¹å¾ (ä¼˜åŒ–é‡ç‚¹)
            edge_array = self._extract_edge_features_batch(frames_rgb)
            
            # æ‹¼æ¥ç‰¹å¾
            combined_features = np.concatenate([efficientnet_array, edge_array], axis=1)
            
            print(f"     âœ… æå–å®Œæˆ: {len(combined_features)} ä¸ªç‰¹å¾å‘é‡ (1344ç»´)")
            
            # â­ é‡‹æ”¾GPUå…§å­˜
            if self.config.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            return combined_features
            
        except Exception as e:
            print(f"âŒ Decord æå–å¤±è´¥: {e}")
            print(f"   å°è¯•ä½¿ç”¨ OpenCV å¤‡ç”¨æ–¹æ³•...")
            return self.extract_video_features_opencv(video_path)


    def extract_video_features_opencv(self, video_path):
        """
        â­ ä¼˜åŒ–ç‰ˆ OpenCV æå– (å…ˆè¯»æ‰€æœ‰å¸§, å†æ‰¹é‡å¤„ç†)
        """
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€å½±ç‰‡: {os.path.basename(video_path)}")
                return None
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            video_fps = cap.get(cv2.CAP_PROP_FPS)
            
            if total_frames == 0:
                print(f"âŒ å½±ç‰‡å¸§æ•°ä¸º0: {os.path.basename(video_path)}")
                cap.release()
                return None
            
            if video_fps <= 0:
                video_fps = 24
            
            aspect_ratio = self._detect_aspect_ratio(video_path)
            duration = total_frames / video_fps
            
            print(f"  ğŸ“¹ {os.path.basename(video_path)}")
            print(f"     æ¯”ä¾‹: {aspect_ratio} | FPS: {video_fps:.1f} | é•¿åº¦: {duration:.1f}ç§’")
            
            sample_indices, actual_frames, process_duration = self._calculate_sample_indices(total_frames, video_fps)
            target_fps = getattr(self.config, 'target_fps', 12)
            print(f"     é‡‡æ ·: {actual_frames} å¸§ ({process_duration:.1f}ç§’ @ {target_fps}FPS)")
            
            # â­ å…ˆè¯»å–æ‰€æœ‰å¸§ (é¿å…é‡å¤seek)
            frames_rgb = []
            for frame_idx in sample_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, min(frame_idx, total_frames-1))
                ret, frame = cap.read()
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames_rgb.append(frame_rgb)
                else:
                    if frames_rgb:
                        frames_rgb.append(frames_rgb[-1])
                    else:
                        h, w = self.config.target_height, self.config.target_width
                        frames_rgb.append(np.zeros((h, w, 3), dtype=np.uint8))
            
            cap.release()
            
            if not frames_rgb:
                print(f"âŒ æœªèƒ½æå–ä»»ä½•å¸§")
                return None
            
            # â­ æ‰¹é‡æå– EfficientNet ç‰¹å¾
            frames_pil = [Image.fromarray(frame) for frame in frames_rgb]
            frame_tensors = [self.transform(pil) for pil in frames_pil]
            
            efficientnet_features = []
            batch_size = 32  # å¢å¤§batch
            
            with torch.no_grad():
                for i in range(0, len(frame_tensors), batch_size):
                    batch_frames = frame_tensors[i:i+batch_size]
                    batch_tensor = torch.stack(batch_frames).to(self.config.device)
                    
                    batch_features = self.feature_extractor(batch_tensor)
                    batch_features = self.global_pool(batch_features)
                    batch_features = batch_features.flatten(1).cpu().numpy()
                    
                    efficientnet_features.extend(batch_features)
            
            efficientnet_array = np.array(efficientnet_features)
            
            # â­ æ‰¹é‡æå–è¾¹ç¼˜ç‰¹å¾
            edge_array = self._extract_edge_features_batch(frames_rgb)
            
            # æ‹¼æ¥ç‰¹å¾
            combined_features = np.concatenate([efficientnet_array, edge_array], axis=1)
            
            print(f"     âœ… æå–å®Œæˆ: {len(combined_features)} ä¸ªç‰¹å¾å‘é‡ (1344ç»´)")
            
            # â­ é‡‹æ”¾GPUå…§å­˜
            if self.config.device.type == 'cuda':
                torch.cuda.empty_cache()
            
            return combined_features
            
        except Exception as e:
            print(f"âŒ OpenCV æå–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def extract_video_features(self, video_path):
        """
        â­ æå–å½±ç‰‡ç‰¹å¾µï¼ˆè‡ªå‹•é¸æ“‡æœ€å¿«æ–¹æ³•ï¼Œè¼¸å‡º1344ç¶­ï¼‰
        """
        if self.use_decord:
            return self.extract_video_features_decord(video_path)
        else:
            return self.extract_video_features_opencv(video_path)
    
    def extract_batch_features(self, video_samples):
        """æ‰¹é‡æå–ç‰¹å¾µ"""
        print("ğŸš€ é–‹å§‹æ‰¹é‡ç‰¹å¾µæå–...")
        print(f"ğŸ“ ç›®æ¨™å°ºå¯¸: {self.config.target_height}x{self.config.target_width}")
        
        target_fps = getattr(self.config, 'target_fps', 12)
        min_duration = getattr(self.config, 'min_video_duration', 2)
        max_duration = getattr(self.config, 'max_video_duration', 15)
        content_retain = getattr(self.config, 'content_retain', 0.92)
        
        print(f"ğŸ¬ æ¡æ¨£ç‡: {target_fps} FPS")
        print(f"â±ï¸  è™•ç†ç¯„åœ: {min_duration}-{max_duration}ç§’")
        print(f"âœ‚ï¸  è™•ç†ç­–ç•¥: æ¯”ä¾‹æ¥è¿‘ç›´æ¥ç¸®æ”¾ï¼Œæ¯”ä¾‹å·®ç•°å¤§æ™‚æ™ºèƒ½è£åˆ‡ï¼ˆä¿ç•™{content_retain*100:.0f}%ä¸­å¿ƒç•«é¢ï¼‰")
        print(f"ğŸ“ ç›®æ¨™ç‰¹å¾µç›®éŒ„: {self.config.feature_dir}")
        print(f"âš¡ ä½¿ç”¨æ–¹æ³•: {'Decord (å¿«é€Ÿ)' if self.use_decord else 'OpenCV (è¼ƒæ…¢)'}")
        print(f"â­ è¼¸å‡ºç¶­åº¦: 1344 (EfficientNet 1280 + é‚Šç·£ 64)")
        print(f"ğŸš€ é‚Šç·£æª¢æ¸¬: PyTorch GPU Canny (åŠ é€Ÿ)")
        
        new_feature_samples = []
        success_count = 0
        
        for video_path, animator in tqdm(video_samples, desc="æå–ç‰¹å¾µ"):
            # ç”Ÿæˆç‰¹å¾µæ–‡ä»¶å
            video_name = os.path.basename(video_path)
            feature_file = f"{os.path.splitext(video_name)[0]}.npy"
            feature_path = os.path.join(self.config.feature_dir, feature_file)
            
            # å¦‚æœç‰¹å¾µå·²å­˜åœ¨ï¼Œè·³é
            if os.path.exists(feature_path):
                new_feature_samples.append((feature_path, animator))
                success_count += 1
                continue
            
            # æå–æ–°ç‰¹å¾µ
            features = self.extract_video_features(video_path)
            if features is not None:
                np.save(feature_path, features)
                new_feature_samples.append((feature_path, animator))
                success_count += 1
        
        print(f"âœ… ç‰¹å¾µæå–å®Œæˆ: {success_count}/{len(video_samples)} æˆåŠŸ")
        return new_feature_samples